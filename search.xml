<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[docker 总结]]></title>
    <url>%2F2020%2F02%2F07%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker 总结 之前大体上玩儿过，但是一直没有完整的使用过；这次借着天池的一个新人赛 【入门】Docker练习场，梳理了一下docke的基本操作。 window下docker解决方案 docker对于unix是比较友好的，无需累述；docker在windows下一直没有好的解决方案，今天花了一天的时间终于找到了最好的解决方案，并且完成了比赛。 的目前为止最好的解决方案还是docker for windows，之前的解决方案可能是VM、或者win10子系统WSL里的docker，wsl虽然很方便也有方法能起来docker，但是很容易碰见这样那样的问题，docker守护进程方面很难弄。之所以说docker for windows是最好的解决方案是因为他的新版本解决了对于hyper-v的依赖，使得他自己与VM等不会发生冲突，这就解决了他之前最大的弊端，使用起来没有复杂的障碍与冲突。详细的进步在知乎：Windows下想使用Linux环境，WSL、Docker、VM应该怎么选择？。 windows下安装也很简单，Welcome to Docker Hub: Download and Take a Tutorial。 记得开启docker的镜像加速，不然下载镜像会很慢，可以使用阿里云的加速，阿里云加速链接。 常用命令 命令 操作 docker images 查看已存在镜像 docker ps 查看正在运行的容器 docker ps -a 查看所有容器 docker run -it [打包的镜像名称]:[tag] bash 启动镜像 docker commit -a “dyngq” -m “test” [容器名称或id] [打包的镜像名称]:[tag] 将容器打包成镜像 docker rm 删除容器 docker rmi 删除镜像 docker bulid -t [打包的镜像名称]:[tag] 根据Dockerfile打包镜像 docker start 启动容器 docker attach 进入容器 将容器打包成镜像docker commit -a “dyngq” -m “test” [容器名称或id] [打包的镜像名称]:[tag] -a :提交的镜像作者；-c :使用Dockerfile指令来创建镜像；-m :提交时的说明文字；-p :在commit时，将容器暂停。 Dockerfile示例Dockfile # Base Images ## 从带有numpy的python镜像 FROM numpy:1.0 ## 把当前文件夹里的文件构建到镜像的根目录下 ADD . / ## 指定默认工作目录为根目录（需要把run.sh和生成的结果文件都放在该文件夹下，提交后才能运行） WORKDIR / ## 镜像启动后统一执行 sh run.sh CMD [&quot;sh&quot;, &quot;run.sh&quot;]其他一些常用参考链接 菜鸟教程的docker教学 docker如何部署您的第一个应用程序 pass 天池 手把手超详细操作说明 创建阿里云的docker仓库 pull拉取提供的python3镜像 启动镜像，在这个容器内安装numpy ( pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple) 将安装有numpy的容器打包成镜像 写好Dockerfile，写好sh和py push上去 提交结果 本次提交的所有文件都在./docker文件夹内 py文件 # import pandas as pd import numpy as np import json # df = pd.read_csv(&quot;/tcdata/num_list.csv&quot;) # df = pd.read_csv(&quot;./docker/tianchi_submit_demo/data/tcdata/num_list.csv&quot;) # print(df) numbers = np.loadtxt(open(&quot;./tcdata/num_list.csv&quot;,&quot;rb&quot;),delimiter=&quot;,&quot;,skiprows=0,dtype=&apos;int&apos;) # numbers = np.loadtxt(open(&quot;./docker/tianchi_submit_demo/data/tcdata/num_list.csv&quot;,&quot;rb&quot;),delimiter=&quot;,&quot;,skiprows=0,dtype=&apos;int&apos;) # numbers = np.random.randint(1,30,size=50,dtype=&apos;int32&apos;) # print(numbers) # np.savetxt(&apos;./docker/tianchi_submit_demo/data/tcdata/num_list.csv&apos;, numbers,delimiter = &apos;,&apos;) # print(&quot;hello_world&quot;) # print(numbers,type(numbers.tolist())) r_sum = np.sum(numbers) top10 = numbers[np.argpartition(numbers,-10)[-10:]] top10 = np.sort(top10).tolist() top10.reverse() # print(top10, type(top10)) result = { &quot;Q1&quot;: &quot;Hello world&quot;, &quot;Q2&quot;: r_sum.tolist(), # C004 注意：TOP10 若包含重复值 &quot;Q3&quot;: top10 } with open(&quot;result.json&quot;, &quot;w&quot;) as f: json.dump(result, f)]]></content>
      <categories>
        <category>Sec</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>Sec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dyngq]]></title>
    <url>%2F2019%2F07%2F21%2Fdyngq%2F</url>
    <content type="text"><![CDATA[dyngq 总览 不管什么时候开始，开始了就不要停止 Notebooks 卷积神经网络总结 stacking总结 统计学习方法笔记 李宏毅机器学习笔记 关于Papers的总结 Papers_NLP方向论文笔记 Papers_CV方向论文笔记 比较完整全面的总结将会放在这一部分，其他的学习思考会放在Daily-log里面 Projects 本科毕业设计：基于深度学习与词嵌入的情感分析系统设计与实现 Competitions 201903_研究生入学考试复试PTA机试_刷题 201904_Datacon大数据安全比赛 201907_绿色计算大赛 Daily_logs 2019-06-05_导师让思考的关于模型对于具体问题的选择问题 2019-06-06_一个印地语的情感分析的预选赛 2019-07-21_关于贝叶斯相关概率论知识的学习与简单总结.md 2019-07-22_服务器搭建博客总结 2019-09-16_尝试破解WiFi密码 但行好事，莫问前程 dyngq]]></content>
      <categories>
        <category>dyngq</category>
      </categories>
      <tags>
        <tag>dyngq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stacking模型融合]]></title>
    <url>%2F2019%2F07%2F21%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89stacking%2F</url>
    <content type="text"><![CDATA[Stacking模型融合 1.介绍一下stacking的原理等 2.机器学习算法进行stacking很常见，像mlxtend等库对stacking算法封装还不错 3.简单实现一下神经网络的stacking算法 最经典的一张图: 但是可以不从这个图开始理解，这个图是一个表较复杂，比较不容易过拟合的一种复杂实现，理解stacking更重要的是理解stacking总体的思想。由浅入深，先从整体上把握，再针对不同的情境复杂优化。 stacking算法基于一个简单的思想，与其使用一些平凡的函数（比如硬投票）来聚合集成所有模型预测，那为什么不训练一个模型来执行这些聚合呢。 总体概述： stacking原则上可以分成若干层级，但层数多不一定效果越好，一般采用两层结构。 因此，我们可以把stacking过程看作是两个级别，级别0和级别1。 0级: 也就是对应第一层，0级模型就是我们需要堆叠的多个模型，也称为预测器。可以是随机森林SVM等等多种模型，也可以只有一种模型。对这些模型在训练集上进行正常训练。 1级: 也就是对应第二层，1级模型称为混合器或元学习器（blender, or a meta learner）。混合器的训练集的输入特征（x）是上一层多个预测器的预测值的拼接，混合器的训练集的预测值（y）就是训练集原来的预测值（y）。 stacking一般采用预留集的方法训练1级模型（元学习器），如图所示，训练集被拆分为2部分（注意，这里不是要几折也不是要拆分成训练集验证集测试集），子集1正常训练模型，得到n个预测器。让刚刚得到的n个预测器在预留集（子集2）上进行预测，（由于之前一直没有见过子集2里的数据，所以可以确保预测是干净的），这样就得到了n个预测值。将这些预测值作为输入特征，创建一个新的训练集（n维），保留原来的label作为label，在这个新的训练集上训练1级模型（元学习器/混合器），让它学习使用预测器的预测来预测目标值。 当然，就像之前说的，stacking可以有多层，比如三层。 第一层是预测器，第二层是多个混合器（混合器可以有多种，随机森林、线性回归等等），第三层是又一层的混合器。 这是就需要讲训练集分成三部分，在三层上“干净的”训练。 原理如下图： 第二部分：下面是网上最常见的stacking方法解释(也就是文章已开始的图片所描述的) (神经网络的stacking应用在下一部分)一种更为复杂的方法是使用k-fold交叉验证来开发元学习机模型的训练数据集，也就是对应一开始的那张图片。每个0级模型预测器都使用k-fold交叉验证(甚至为了达到最大效果使用留一法交叉验证)进行训练;然后模型被丢弃，但是预测被保留。这意味着对于每个模型，都有一个模型版本所做的预测，而这个版本的模型并没有针对这些例子进行训练，例如，有一些在预留的例子。关于K折交叉验证 下面是比较好最普遍的解释（来自网上，文末链接）： 对于每一轮的 5-fold，Model 1都要做满5次的训练和预测。Titanic 栗子：Train Data有890行。(请对应图中的上层部分）每1次的fold，都会生成 713行 小train， 178行 小test。我们用Model 1来训练 713行的小train，然后预测 178行 小test。预测的结果是长度为 178 的预测值。这样的动作走5次！ 长度为178 的预测值 X 5 = 890 预测值，刚好和Train data长度吻合。这个890预测值是Model 1产生的，我们先存着，因为，一会让它将是第二层模型的训练来源。重点：这一步产生的预测值我们可以转成 890 X 1 （890 行，1列），记作 P1 (大写P)接着说 Test Data 有 418 行。(请对应图中的下层部分，对对对，绿绿的那些框框）每1次的fold，713行 小train训练出来的Model 1要去预测我们全部的Test Data（全部！因为Test Data没有加入5-fold，所以每次都是全部！）。此时，Model 1的预测结果是长度为418的预测值。这样的动作走5次！我们可以得到一个 5 X 418 的预测值矩阵。然后我们根据行来就平均值，最后得到一个 1 X 418 的平均预测值。重点：这一步产生的预测值我们可以转成 418 X 1 （418行，1列），记作 p1 (小写p)走到这里，你的第一层的Model 1完成了它的使命。第一层还会有其他Model的，比如Model 2，同样的走一遍， 我们有可以得到 890 X 1 (P2) 和 418 X 1 (p2) 列预测值。这样吧，假设你第一层有3个模型，这样你就会得到：来自5-fold的预测值矩阵 890 X 3，（P1，P2， P3） 和 来自Test Data预测值矩阵 418 X 3， （p1, p2, p3）。 到第二层了………………来自5-fold的预测值矩阵 890 X 3 作为你的Train Data，训练第二层的模型来自Test Data预测值矩阵 418 X 3 就是你的Test Data，用训练好的模型来预测他们吧。 最后 ，放出一张Python的Code，在网上为数不多的stacking内容里， 这个几行的code你也早就看过了吧，我之前一直卡在这里，现在加上一点点注解，希望对你有帮助： 第三部分：神经网络的stacking 有一篇专门的英文博客介绍了传统机器学习以及神经网络的stacking，文末链接。 做了京东评论的情感分析，尝试使用了stacking。 后来毕业设计老师建议使用英文数据集，相对于中文噪声小等，所以以后还会尝试在IMDB和Twitter数据集上的复杂神经网络模型上进行stacking。 参考资料： 1.最完整的，包括深度学习 中文译文：https://blog.csdn.net/LaoChengZier/article/details/86504464 英文原版：https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/ 2.https://www.leiphone.com/news/201709/zYIOJqMzR0mJARzj.html 3.https://blog.csdn.net/willduan1/article/details/73618677 4.]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>Ensemble</tag>
        <tag>Stacking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络 - Convolutional Neural Network- CNN]]></title>
    <url>%2F2019%2F06%2F06%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20-%20Convolutional%20Neural%20Network-%20CNN%2F</url>
    <content type="text"><![CDATA[卷积神经网络 - Convolutional Neural Network- CNN1.应用场景： 卷积神经网络是用于计算机视觉任务的最佳机器学习模型。即使在非常小的数据集上也可以从头开始训练一个卷积神经网络，而且得到的结果还不错。 用于图像分类问题。数据集肯定越大越好，但是CNN可以处理那些训练集较小的问题 组成：卷积层 池化层 它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。它包括卷积层和池化层。此外一般还有需要分类器，在预训练中一般会冻结卷积基（卷积+池化），通过调节训练分类器实现在小规模数据集上的训练精度提高。 一般用最大池化层，原因在于特征中往往编码了某种模式或概念在特征图的不同位置是否存在（因此得名特征图），而观察不同特征的最大值而不是平均值能够给出更多的信息。 卷积层不同点 密集连接层和卷积层的根本区别在于， Dense 层从输入特征空间中学到的是全局模式，而卷积层学到的是局部模式。图像可以被分解为局部模式，如边缘、纹理等。 2.卷积神经网络具有两个有趣的性质 平移不变性 卷积神经网络学到的模式具有平移不变性（translation invariant）。 卷积神经网络在图像右下角学到某个模式之后，它可以在任何地方识别这个模式，比如左上角。 这使得卷积神经网络在处理图像时可以高效利用数据（因为视觉世界从根本上具有平移不变性），它只需要更少的训练样本就可以学到具有泛化能力的数据表示。 CNN可以学到模式的空间层次结构 第一个卷积层将学习较小的局部模式（比如边缘），第二个卷积层将学习由第一层特征组成的更大的模式，以此类推。 这使得卷积神经网络可以有效地学习越来越复杂、越来越抽象的视觉概念（因为视觉世界从根本上具有空间层次结构）。 3.特征图 3D张量（高度宽度通道数）的卷积也叫特征图 含义：深度轴的每个纬度都是一个特征（或者说是过滤器） 卷积运算，输入特征图，从输入特征图中提取图块，并对所有这些图块应用相同的变换，生成输出特征图 该输出特征图仍是一个 3D 张量，具有宽度和高度，其深度可以任意取值，因为输出深度是层的参数，深度轴的不同通道不再像 RGB 输入那样代表特定颜色，而是代表过滤器（filter）。过滤器对输入数据的某一方面进行编码。 4.卷积由两个关键参数所定义 从输入中提取的图块尺寸,通常是 3×3 或 5×5. 输出特征图的深度：卷积所计算的过滤器的数量。 5.1 卷积的工作原理 在 3D 输入特征图上滑动（slide）这些 3×3 或 5×5 的窗口，在每个可能的位置停止并提取周围特征的 3D 图块［形状为 (window_height, window_width, input_depth) ］。 然后每个3D 图块与学到的同一个权重矩阵［叫作卷积核（convolution kernel）］做张量积，转换成形状为 (output_depth,) 的 1D 向量。 然后对所有这些向量进行空间重组，使其转换为形状为 (height, width, output_depth) 的 3D 输出特征图。 输出特征图中的每个空间位置都对应于输入特征图中的相同位置（比如输出的右下角包含了输入右下角的信息）。 输出的宽度和高度可能与输入的宽度和高度不同。不同的原因可能有两点。 边界效应，可以通过对输入特征图进行填充来抵消。 使用了步幅（stride）。卷积步幅，步进卷积。 Gif动图说明。 5.2 添加非线性激活ReLU（修正线性单元）层 在每个卷积层之后，通常会立即应用一个非线性层（或激活层）。其目的是给一个在卷积层中刚经过线性计算操作（只是数组元素依次（element wise）相乘与求和）的系统引入非线性特征。过去，人们用的是像双曲正切和 S 型函数这样的非线性方程，但研究者发现 ReLU 层效果好得多，因为神经网络能够在准确度不发生明显改变的情况下把训练速度提高很多（由于计算效率增加）。它同样能帮助减轻梯度消失的问题——由于梯度以指数方式在层中消失，导致网络较底层的训练速度非常慢。ReLU 层对输入内容的所有值都应用了函数 f(x) = max(0, x)。用基本术语来说，这一层把所有的负激活（negative activation）都变为零。这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。（参见 Geoffrey Hinton（即深度学习之父）的论文：Rectified Linear Units Improve Restricted Boltzmann Machines） 此部分参考链接 5.3 感受野 感受野 常用一种特殊方法，如下图右边一列所示，卷积之后位置不变，空白部分进行填充的方法。 而感受野就是右列的 以每一个实体位置为中心 对应的输入图的信息映射的包含的部分。 参考链接,对CNN感受野一些理解 6.最大池化 在每个 MaxPooling2D 层之后，特征图的尺寸都会减半。 最大池化的作用：对特征图进行下采样，与步进卷积类似。 使用下采样的原因 一是减少需要处理的特征图的元素个数 二是通过让连续卷积层的观察窗口越来越大（即窗口覆盖原始输入的比例越来越大），从而引入空间过滤器的层级结构。 7.1 在小型数据集上从头开始训练一个卷积神经网络 数据集越大一般相对越好，只用几十个样本训练卷积神经网络就解决一个复杂问题是不可能的，但如果模型很小，并做了很好的正则化，同时任务非常简单，那么几百个样本可能就足够了。 讨论猫狗图像分类，数据集中包含 4000 张猫和狗的图像（2000 张猫的图像，2000 张狗的图像）。我们将 2000 张图像用于训练，1000 张用于验证，1000张用于测试。 模型 可以看到： 由于边界效应：每个卷积层之后，就减少两行两列 由于最大池化，每个池化层之后，缩小为原来的一半 数据预处理 读取图像文件。 将 JPEG 文件解码为 RGB 像素网格。 将这些像素网格转换为浮点数张量。 将像素值（0~255 范围内）缩放到 [0, 1] 区间（神经网络喜欢处理较小的输入值）。 Keras 拥有自动完成这些步骤的工具，它包含ImageDataGenerator 类，可以快速创建 Python 生成器，能够将硬盘上的图像文件自动转换为预处理好的张量批量。 训练好模型后，保存模型 通过可视化两个图像 训练精度和验证精度 和 训练损失和验证损失，发现第几轮开始过拟合。 因为训练样本相对较少（2000 个），所以过拟合是最关心的问题。 解决过拟合1.正则化 dropout 权重衰减（L2 正则化） 2.数据增强 其方法是利用多种能够生成可信图像的随机变换来增加（augment）样本。 其目标是，模型在训练时不会两次查看完全相同的图像。 这让模型能够观察到数据的更多内容，从而具有更好的泛化能力。 在 Keras 中，这可以通过对 ImageDataGenerator 实例读取的图像执行多次随机变换来实现 需要注意的是，不能增强验证数据 3.使用预训练的卷积神经网络 预训练网络（pretrained network）是一个保存好的网络，之前已在大型数据集（通常是大规模图像分类任务）上训练好。如果这个原始数据集足够大且足够通用，那么预训练网络学到的特征的空间层次结构可以有效地作为视觉世界的通用模型，因此这些特征可用于各种不同的计算机视觉问题，即使这些新问题涉及的类别和原始任务完全不同。 使用预训练网络有两种方法：特征提取（feature extraction）和微调模型（fine-tuning）。 3.1 特征提取 对于卷积神经网络而言，特征提取就是取出之前训练好的网络的卷积基，在上面运行新数据，然后在输出上面训练一个新的分类器 卷积基学到的表示可能更加通用，因此更适合重复使用。所以，仅重复使用卷积基，训练密集连接分类器。 特征提取有两种方法 不使用数据增强的快速特征提取 直接调用 conv_base 模型的 predict 方法来从这些图像中提取特征，将输出保存成硬盘中的 Numpy 数组，然后用这个数据作为输入，输入到独立的密集连接分类器中（注意要使用 dropout 正则化），计算量很小，计算代价低。 使用数据增强的特征提取 扩展 conv_base 模型，然后在输入数据上端到端地运行模型。 新定义的模型不只有Dense层，也包括了conv_base模型，但是这个模型卷积基需要冻结（freeze），因为如果不冻结的话，那么卷积基之前学到的表示将会在训练过程中被修改。 3.2 微调模型 对于用于特征提取的冻结的模型基，微调是指将其顶部的几层“解冻”，并将这解冻的几层和新增加的部分（本例中是全连接分类器）联合训练（见图 5-19）。之所以叫作微调，是因为它只是略微调整了所复用模型中更加抽象的表示，以便让这些表示与手头的问题更加相关。 之所以只解冻微调模型底部的一小部分层，是因为： 卷积基中更靠底部的层编码的是更加通用的可复用特征，而更靠顶部的层编码的是更专业化的特征。微调这些更专业化的特征更加有用，因为它们需要在你的新问题上改变用途。微调更靠底部的层，得到的回报会更少。 训练的参数越多，过拟合的风险越大。卷积基有 1500 万个参数，所以在你的小型数据集上训练这么多参数是有风险的。 卷积神经网络的可视化虽然对于某些类型的深度学习模型来说，深度学习模型是“黑盒”，即模型学到的表示很难用人类可以理解的方式来提取和呈现。但对卷积神经网络来说绝对不是这样。卷积神经网络学到的表示非常适合可视化，很大程度上是因为它们是视觉概念的表示。 可视化卷积神经网络的中间输出（中间激活）：有助于理解卷积神经网络连续的层如何对输入进行变换，也有助于初步了解卷积神经网络每个过滤器的含义。 可视化卷积神经网络的过滤器：有助于精确理解卷积神经网络中每个过滤器容易接受的视觉模式或视觉概念。 可视化图像中类激活的热力图：有助于理解图像的哪个部分被识别为属于某个类别，从而可以定位图像中的物体。 1.0 2.0 随着层数的加深，卷积神经网络中的过滤器变得越来越复杂，越来越精细。 模型第一层（ block1_conv1 ）的过滤器对应简单的方向边缘和颜色（还有一些是彩色边缘）。 block2_conv1 层的过滤器对应边缘和颜色组合而成的简单纹理。 更高层的过滤器类似于自然图像中的纹理：羽毛、眼睛、树叶等。 3.0 参考资料 《python深度学习》(《deep learning with python》(by Francois Chollet)) 秒懂各种深度CNN操作-机器学习算法与Python学习 卷积神经网络（CNN）中卷积的实现 CNN 理解神经网络中卷积(大小，通道数，深度) CNN（卷积神经网络）是什么？有何入门简介或文章吗？ 机器之心 对CNN感受野一些理解]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello-world]]></title>
    <url>%2F2019%2F06%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2019%2F06%2F06%2Fpytorch%2F</url>
    <content type="text"><![CDATA[pytorchTips Parameter类其实是Tensor的子类 产生分布的函数 函数 功能 tensor.uniform_(-10, 10) 均匀分布 tensor.normal_(mean, std) 标准正态分布 一些基本操作 函数 功能 trace 对角线元素之和(矩阵的迹) diag 对角线元素 triu/tril 矩阵的上三角/下三角，可指定偏移量 mm/bmm 矩阵乘法，batch的矩阵乘法 addmm/addbmm/addmv/addr/baddbmm.. 矩阵运算 t 转置 dot/cross 内积/外积 inverse 求逆矩阵 svd 奇异值分解 PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。 Ques log_softmax比softmax多计算一次log，意义在于 加快计算速度，数值上也更稳定。 参考资料：PyTorch学习笔记——softmax和log_softmax的区别、CrossEntropyLoss() 与 NLLLoss() 的区别、log似然代价函数 Pytorch中torch.nn.Softmax的dim参数含义 就是在第几维上 sum=1 tf.nn.softmax中dim默认为-1,即,tf.nn.softmax会以最后一个维度作为一维向量计算softmax 注意：tf.nn.softmax函数默认（dim=-1）是对张量最后一维的shape=(p,)向量进行softmax计算，得到一个概率向量。不同的是,tf.nn.sigmoid函数对一个张量的每一个标量元素求得一个概率。也就是说tf.nn.softmax默认针对1阶张量进行运算,可以通过指定dim来针对1阶以上的张量进行运算,但不能对0阶张量进行运算。而tf.nn.sigmoid是针对0阶张量,。 参考资料：tensorflow中交叉熵系列函数 ？？？？ python 深拷贝、浅拷贝 mean std(标准差) ？？？？ numpy.triu torch.from_numpy ？？？？ 负的维度的使用 ？？？？ torch.view .transpose ？？？？ 标签平滑 KL散度评价]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
</search>
